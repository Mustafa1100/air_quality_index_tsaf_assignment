{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Air Quality Time Series Prediction with Gray Model (GM(1,1))\n",
        "\n",
        "This notebook implements **Gray Model (GM(1,1))** for time series forecasting of PM 2.5 air quality.\n",
        "\n",
        "## Key Concepts:\n",
        "- **Gray Model (GM(1,1))**: A grey system theory model ideal for small datasets\n",
        "  - **GM(1,1)**: First-order, one-variable grey model\n",
        "  - Uses Accumulated Generating Operation (AGO) to build the model\n",
        "  - Excellent for small sample sizes (typically 4-10 data points minimum)\n",
        "- **Sliding Windows**: Creating features from past time steps for forecasting\n",
        "- **Time Series Forecasting**: Using historical patterns to predict future PM 2.5 levels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from scipy import stats\n",
        "from scipy.stats import jarque_bera, probplot\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore the Data\n",
        "\n",
        "**Note**: Gray Model works best with small datasets. We'll use a subset of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Real_Combine.csv')\n",
        "\n",
        "print(\"Full Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# For Gray Model, we'll use a smaller subset since it's designed for small datasets\n",
        "# Let's take the first 200-300 points for better GM performance\n",
        "subset_size = 250\n",
        "df_small = df.head(subset_size).copy()\n",
        "\n",
        "print(f\"Using subset of data: {len(df_small)} samples (optimal for Gray Model)\")\n",
        "print(f\"\\nData shape: {df_small.shape}\")\n",
        "print(f\"\\nMissing values: {df_small.isnull().sum().sum()}\")\n",
        "print(f\"\\nData statistics:\")\n",
        "print(df_small['PM 2.5'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Time Series Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the PM 2.5 time series\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(df_small.index, df_small['PM 2.5'], linewidth=1.5, color='steelblue', marker='o', markersize=3)\n",
        "plt.title('PM 2.5 Time Series (Subset for Gray Model)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Time Index', fontsize=12)\n",
        "plt.ylabel('PM 2.5', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Gray Model (GM(1,1)) Implementation\n",
        "\n",
        "The Gray Model uses:\n",
        "1. **Accumulated Generating Operation (AGO)**: Creates cumulative sum\n",
        "2. **Grey Differential Equation**: Models the AGO sequence\n",
        "3. **Inverse AGO (IAGO)**: Converts back to original scale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GrayModel:\n",
        "    \"\"\"\n",
        "    Gray Model (GM(1,1)) implementation for time series forecasting\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.a = None  # Development coefficient\n",
        "        self.b = None  # Grey input\n",
        "        self.x0 = None  # Original sequence\n",
        "        self.x1 = None  # AGO sequence\n",
        "        \n",
        "    def fit(self, x0):\n",
        "        \"\"\"\n",
        "        Fit the Gray Model\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        x0 : array-like\n",
        "            Original time series data\n",
        "        \"\"\"\n",
        "        x0 = np.array(x0).flatten()\n",
        "        self.x0 = x0\n",
        "        \n",
        "        # Step 1: Accumulated Generating Operation (AGO)\n",
        "        self.x1 = np.cumsum(x0)\n",
        "        \n",
        "        # Step 2: Build background values (mean of consecutive AGO values)\n",
        "        n = len(x0)\n",
        "        z1 = np.zeros(n - 1)\n",
        "        for i in range(n - 1):\n",
        "            z1[i] = 0.5 * (self.x1[i] + self.x1[i + 1])\n",
        "        \n",
        "        # Step 3: Build matrices for least squares\n",
        "        B = np.zeros((n - 1, 2))\n",
        "        Y = np.zeros(n - 1)\n",
        "        \n",
        "        for i in range(n - 1):\n",
        "            B[i, 0] = -z1[i]\n",
        "            B[i, 1] = 1\n",
        "            Y[i] = x0[i + 1]\n",
        "        \n",
        "        # Step 4: Solve for parameters using least squares\n",
        "        # [a, b]^T = (B^T * B)^(-1) * B^T * Y\n",
        "        try:\n",
        "            params = np.linalg.lstsq(B, Y, rcond=None)[0]\n",
        "            self.a = params[0]\n",
        "            self.b = params[1]\n",
        "        except:\n",
        "            # Fallback if matrix is singular\n",
        "            self.a = -0.01\n",
        "            self.b = x0.mean()\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, steps=1, start=0):\n",
        "        \"\"\"\n",
        "        Predict future values (in-sample)\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        steps : int\n",
        "            Number of steps ahead to predict\n",
        "        start : int\n",
        "            Starting index for prediction\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        predictions : array\n",
        "            Predicted values\n",
        "        \"\"\"\n",
        "        if self.a is None or self.b is None:\n",
        "            raise ValueError(\"Model must be fitted before prediction\")\n",
        "        \n",
        "        predictions = []\n",
        "        \n",
        "        for i in range(steps):\n",
        "            k = start + i + 1  # k is 1-indexed in GM(1,1) formula\n",
        "            # GM(1,1) AGO prediction: x^(1)(k) = (x^(0)(1) - b/a) * exp(-a*(k-1)) + b/a\n",
        "            x1_k = (self.x0[0] - self.b / self.a) * np.exp(-self.a * (k - 1)) + self.b / self.a\n",
        "            \n",
        "            if k == 1:\n",
        "                # First value is the original first value\n",
        "                pred = self.x0[0]\n",
        "            else:\n",
        "                # IAGO: x^(0)(k) = x^(1)(k) - x^(1)(k-1)\n",
        "                x1_k_minus_1 = (self.x0[0] - self.b / self.a) * np.exp(-self.a * (k - 2)) + self.b / self.a\n",
        "                pred = x1_k - x1_k_minus_1\n",
        "            \n",
        "            predictions.append(pred)\n",
        "        \n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def forecast(self, steps=1):\n",
        "        \"\"\"\n",
        "        Forecast future values (out-of-sample)\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        forecasts : array\n",
        "            Forecasted values\n",
        "        \"\"\"\n",
        "        if self.a is None or self.b is None:\n",
        "            raise ValueError(\"Model must be fitted before forecasting\")\n",
        "        \n",
        "        n = len(self.x0)\n",
        "        forecasts = []\n",
        "        \n",
        "        for i in range(steps):\n",
        "            k = n + i + 1  # k is 1-indexed, starting from n+1\n",
        "            # GM(1,1) AGO forecast: x^(1)(k) = (x^(0)(1) - b/a) * exp(-a*(k-1)) + b/a\n",
        "            x1_k = (self.x0[0] - self.b / self.a) * np.exp(-self.a * (k - 1)) + self.b / self.a\n",
        "            \n",
        "            # IAGO: x^(0)(k) = x^(1)(k) - x^(1)(k-1)\n",
        "            if k == n + 1:\n",
        "                # First forecast uses last known AGO value\n",
        "                x1_k_minus_1 = self.x1[-1]\n",
        "            else:\n",
        "                x1_k_minus_1 = (self.x0[0] - self.b / self.a) * np.exp(-self.a * (k - 2)) + self.b / self.a\n",
        "            \n",
        "            forecast = x1_k - x1_k_minus_1\n",
        "            forecasts.append(forecast)\n",
        "        \n",
        "        return np.array(forecasts)\n",
        "\n",
        "print(\"Gray Model class implemented successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sliding Window Function\n",
        "\n",
        "Create sliding windows for time series prediction using Gray Model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sliding_windows_gm(data, window_size, forecast_horizon=1):\n",
        "    \"\"\"\n",
        "    Create sliding windows for Gray Model time series prediction.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : array-like\n",
        "        Time series data\n",
        "    window_size : int\n",
        "        Number of past time steps to use as features (window size)\n",
        "    forecast_horizon : int\n",
        "        Number of steps ahead to predict (default=1)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    windows : list\n",
        "        List of window arrays\n",
        "    targets : list\n",
        "        List of target arrays\n",
        "    \"\"\"\n",
        "    windows = []\n",
        "    targets = []\n",
        "    \n",
        "    data = np.array(data).flatten()\n",
        "    n_samples = len(data)\n",
        "    \n",
        "    # Create sliding windows\n",
        "    for i in range(window_size, n_samples - forecast_horizon + 1):\n",
        "        # Features: past window_size time steps\n",
        "        window = data[i-window_size:i]\n",
        "        # Target: future forecast_horizon time steps\n",
        "        target = data[i:i+forecast_horizon]\n",
        "        \n",
        "        windows.append(window)\n",
        "        targets.append(target)\n",
        "    \n",
        "    return windows, targets\n",
        "\n",
        "print(\"Sliding window function for Gray Model created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Data for Sliding Windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract PM 2.5 time series\n",
        "ts = df_small['PM 2.5'].values\n",
        "\n",
        "print(f\"Time series length: {len(ts)}\")\n",
        "print(f\"Time series statistics:\")\n",
        "print(f\"  Mean: {ts.mean():.4f}\")\n",
        "print(f\"  Std: {ts.std():.4f}\")\n",
        "print(f\"  Min: {ts.min():.4f}\")\n",
        "print(f\"  Max: {ts.max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Sliding Windows\n",
        "\n",
        "We'll experiment with different window sizes. For Gray Model, smaller windows (5-15) work well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define window size (number of past time steps to use)\n",
        "# For Gray Model, smaller windows (5-10) are typically used\n",
        "window_size = 8  # Using 8 past time steps\n",
        "forecast_horizon = 1  # Predicting 1 step ahead\n",
        "\n",
        "# Create sliding windows\n",
        "windows, targets = create_sliding_windows_gm(ts, window_size=window_size, forecast_horizon=forecast_horizon)\n",
        "\n",
        "print(f\"Number of windows created: {len(windows)}\")\n",
        "print(f\"Window size: {window_size}\")\n",
        "print(f\"Forecast horizon: {forecast_horizon}\")\n",
        "print(f\"\\nExample window (first window):\")\n",
        "print(f\"  Values: {windows[0]}\")\n",
        "print(f\"  Target: {targets[0]}\")\n",
        "print(f\"\\nThis means:\")\n",
        "print(f\"  - We use data from time steps 0 to {window_size-1} to predict time step {window_size}\")\n",
        "print(f\"  - We use data from time steps 1 to {window_size} to predict time step {window_size+1}\")\n",
        "print(f\"  - And so on...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train-Test Split for Time Series\n",
        "\n",
        "**Important**: For time series, we must use chronological split (not random) to preserve temporal order.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series split: use first 80% for training, last 20% for testing\n",
        "# IMPORTANT: Both train and test sets use sliding windows!\n",
        "split_index = int(len(windows) * 0.8)\n",
        "\n",
        "train_windows = windows[:split_index]\n",
        "test_windows = windows[split_index:]\n",
        "train_targets = targets[:split_index]\n",
        "test_targets = targets[split_index:]\n",
        "\n",
        "print(f\"Training set: {len(train_windows)} windows (each with {window_size} past time steps)\")\n",
        "print(f\"Test set: {len(test_windows)} windows (each with {window_size} past time steps)\")\n",
        "print(f\"\\nTraining period: windowed samples 0 to {split_index-1}\")\n",
        "print(f\"Test period: windowed samples {split_index} to {len(windows)-1}\")\n",
        "print(f\"\\n✓ Both training and testing use sliding windows with window_size={window_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train Gray Model on Sliding Windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Gray Model on each training window and make predictions\n",
        "train_predictions = []\n",
        "train_actuals = []\n",
        "\n",
        "print(\"Training Gray Model on sliding windows...\")\n",
        "for i, window in enumerate(train_windows):\n",
        "    try:\n",
        "        # Fit Gray Model on this window\n",
        "        gm = GrayModel()\n",
        "        gm.fit(window)\n",
        "        \n",
        "        # Predict the next value (in-sample)\n",
        "        # For in-sample, we predict the value at position len(window) using start=len(window)-1\n",
        "        pred = gm.predict(steps=1, start=len(window)-1)[0]\n",
        "        train_predictions.append(pred)\n",
        "        train_actuals.append(train_targets[i][0])\n",
        "    except Exception as e:\n",
        "        # If model fails, use mean of window\n",
        "        train_predictions.append(np.mean(window))\n",
        "        train_actuals.append(train_targets[i][0])\n",
        "\n",
        "train_predictions = np.array(train_predictions)\n",
        "train_actuals = np.array(train_actuals)\n",
        "\n",
        "print(f\"Training completed!\")\n",
        "print(f\"Number of training predictions: {len(train_predictions)}\")\n",
        "print(f\"Model parameters (last window):\")\n",
        "if len(train_windows) > 0:\n",
        "    try:\n",
        "        gm_last = GrayModel()\n",
        "        gm_last.fit(train_windows[-1])\n",
        "        print(f\"  a (development coefficient): {gm_last.a:.6f}\")\n",
        "        print(f\"  b (grey input): {gm_last.b:.6f}\")\n",
        "    except:\n",
        "        print(\"  Could not extract parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Make Predictions on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set using sliding windows\n",
        "test_predictions = []\n",
        "test_actuals = []\n",
        "\n",
        "print(\"Making predictions on test set...\")\n",
        "for i, window in enumerate(test_windows):\n",
        "    try:\n",
        "        # Fit Gray Model on this window\n",
        "        gm = GrayModel()\n",
        "        gm.fit(window)\n",
        "        \n",
        "        # Forecast the next value (out-of-sample)\n",
        "        forecast = gm.forecast(steps=1)[0]\n",
        "        test_predictions.append(forecast)\n",
        "        test_actuals.append(test_targets[i][0])\n",
        "    except Exception as e:\n",
        "        # If model fails, use mean of window\n",
        "        test_predictions.append(np.mean(window))\n",
        "        test_actuals.append(test_targets[i][0])\n",
        "\n",
        "test_predictions = np.array(test_predictions)\n",
        "test_actuals = np.array(test_actuals)\n",
        "\n",
        "print(f\"Predictions completed!\")\n",
        "print(f\"Number of test predictions: {len(test_predictions)}\")\n",
        "print(f\"\\nFirst few predictions:\")\n",
        "for i in range(min(5, len(test_predictions))):\n",
        "    print(f\"  Actual: {test_actuals[i]:.4f}, Predicted: {test_predictions[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate evaluation metrics\n",
        "train_mae = mean_absolute_error(train_actuals, train_predictions)\n",
        "train_mse = mean_squared_error(train_actuals, train_predictions)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "train_r2 = r2_score(train_actuals, train_predictions)\n",
        "\n",
        "test_mae = mean_absolute_error(test_actuals, test_predictions)\n",
        "test_mse = mean_squared_error(test_actuals, test_predictions)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_r2 = r2_score(test_actuals, test_predictions)\n",
        "\n",
        "# Display results\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL EVALUATION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n{'Metric':<20} {'Training':<20} {'Test':<20}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'R² Score':<20} {train_r2:<20.4f} {test_r2:<20.4f}\")\n",
        "print(f\"{'MAE':<20} {train_mae:<20.4f} {test_mae:<20.4f}\")\n",
        "print(f\"{'MSE':<20} {train_mse:<20.4f} {test_mse:<20.4f}\")\n",
        "print(f\"{'RMSE':<20} {train_rmse:<20.4f} {test_rmse:<20.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.5. Residual Diagnostic Plots\n",
        "\n",
        "**Checking the Four Key Assumptions of Regression:**\n",
        "\n",
        "1. **Linearity**: The relationship between predictors and response is linear\n",
        "2. **Independence**: Residuals are independent (especially important for time series)\n",
        "3. **Homoscedasticity**: Constant variance of residuals across all fitted values\n",
        "4. **Normality**: Residuals are normally distributed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate residuals for both train and test sets\n",
        "train_residuals = train_actuals - train_predictions\n",
        "test_residuals = test_actuals - test_predictions\n",
        "\n",
        "# Combine for overall analysis\n",
        "all_residuals = np.concatenate([train_residuals, test_residuals])\n",
        "all_fitted = np.concatenate([train_predictions, test_predictions])\n",
        "all_order = np.arange(len(all_residuals))\n",
        "\n",
        "print(\"Residuals calculated for diagnostic analysis\")\n",
        "print(f\"Train residuals: mean={np.mean(train_residuals):.4f}, std={np.std(train_residuals):.4f}\")\n",
        "print(f\"Test residuals: mean={np.mean(test_residuals):.4f}, std={np.std(test_residuals):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive residual diagnostic plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Residual Diagnostic Plots for Gray Model (GM(1,1))', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# ============================================\n",
        "# 1. Normal Probability Plot (Q-Q Plot) - Checking NORMALITY\n",
        "# ============================================\n",
        "probplot(all_residuals, dist=\"norm\", plot=axes[0, 0])\n",
        "axes[0, 0].set_title('Normal Probability Plot of Residuals\\n(Checking Normality)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Theoretical Quantiles', fontsize=10)\n",
        "axes[0, 0].set_ylabel('Sample Quantiles (Residuals)', fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Perform normality tests\n",
        "if len(all_residuals) > 0:\n",
        "    shapiro_stat, shapiro_p = stats.shapiro(all_residuals[:5000]) if len(all_residuals) > 5000 else stats.shapiro(all_residuals)\n",
        "    jb_stat, jb_p = jarque_bera(all_residuals)\n",
        "    \n",
        "    axes[0, 0].text(0.05, 0.95, \n",
        "                    f'Shapiro-Wilk p-value: {shapiro_p:.4f}\\nJarque-Bera p-value: {jb_p:.4f}',\n",
        "                    transform=axes[0, 0].transAxes,\n",
        "                    verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "                    fontsize=9)\n",
        "    \n",
        "    if shapiro_p > 0.05:\n",
        "        axes[0, 0].text(0.95, 0.05, '✓ Normality OK', \n",
        "                       transform=axes[0, 0].transAxes,\n",
        "                       verticalalignment='bottom',\n",
        "                       horizontalalignment='right',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
        "                       fontsize=10, fontweight='bold')\n",
        "    else:\n",
        "        axes[0, 0].text(0.95, 0.05, '✗ Normality Violated', \n",
        "                       transform=axes[0, 0].transAxes,\n",
        "                       verticalalignment='bottom',\n",
        "                       horizontalalignment='right',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7),\n",
        "                       fontsize=10, fontweight='bold')\n",
        "\n",
        "# ============================================\n",
        "# 2. Residuals vs Fitted Values - Checking LINEARITY & HOMOSCEDASTICITY\n",
        "# ============================================\n",
        "scatter1 = axes[0, 1].scatter(all_fitted, all_residuals, alpha=0.6, s=30, c=all_order, \n",
        "                              cmap='viridis', edgecolors='black', linewidth=0.5)\n",
        "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Residual Line')\n",
        "axes[0, 1].set_title('Residuals vs Fitted Values\\n(Checking Linearity & Homoscedasticity)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Fitted Values (Predicted PM 2.5)', fontsize=10)\n",
        "axes[0, 1].set_ylabel('Residuals (Actual - Predicted)', fontsize=10)\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Levene's test for equal variances\n",
        "try:\n",
        "    n_regions = 5\n",
        "    fitted_sorted_idx = np.argsort(all_fitted)\n",
        "    region_size = len(all_fitted) // n_regions\n",
        "    region_groups = []\n",
        "    for i in range(n_regions):\n",
        "        start_idx = i * region_size\n",
        "        end_idx = (i + 1) * region_size if i < n_regions - 1 else len(all_fitted)\n",
        "        region_groups.append(all_residuals[fitted_sorted_idx[start_idx:end_idx]])\n",
        "    \n",
        "    levene_stat, levene_p = stats.levene(*region_groups)\n",
        "    axes[0, 1].text(0.05, 0.95, \n",
        "                   f'Levene Test p-value: {levene_p:.4f}',\n",
        "                   transform=axes[0, 1].transAxes,\n",
        "                   verticalalignment='top',\n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "                   fontsize=9)\n",
        "    \n",
        "    if levene_p > 0.05:\n",
        "        axes[0, 1].text(0.95, 0.05, '✓ Homoscedasticity OK', \n",
        "                       transform=axes[0, 1].transAxes,\n",
        "                       verticalalignment='bottom',\n",
        "                       horizontalalignment='right',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
        "                       fontsize=10, fontweight='bold')\n",
        "    else:\n",
        "        axes[0, 1].text(0.95, 0.05, '✗ Heteroscedasticity Detected', \n",
        "                       transform=axes[0, 1].transAxes,\n",
        "                       verticalalignment='bottom',\n",
        "                       horizontalalignment='right',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7),\n",
        "                       fontsize=10, fontweight='bold')\n",
        "except:\n",
        "    levene_p = None\n",
        "\n",
        "# ============================================\n",
        "# 3. Histogram of Residuals - Checking NORMALITY\n",
        "# ============================================\n",
        "n, bins, patches = axes[1, 0].hist(all_residuals, bins=30, edgecolor='black', \n",
        "                                   linewidth=1.2, alpha=0.7, color='steelblue', density=True)\n",
        "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Line')\n",
        "\n",
        "# Overlay normal distribution\n",
        "mu, sigma = np.mean(all_residuals), np.std(all_residuals)\n",
        "x_norm = np.linspace(all_residuals.min(), all_residuals.max(), 100)\n",
        "y_norm = stats.norm.pdf(x_norm, mu, sigma)\n",
        "axes[1, 0].plot(x_norm, y_norm, 'r-', linewidth=2, label=f'Normal(μ={mu:.2f}, σ={sigma:.2f})')\n",
        "axes[1, 0].set_title('Histogram of Residuals\\n(Checking Normality)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Residuals', fontsize=10)\n",
        "axes[1, 0].set_ylabel('Density', fontsize=10)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add skewness and kurtosis\n",
        "skewness = stats.skew(all_residuals)\n",
        "kurtosis = stats.kurtosis(all_residuals)\n",
        "axes[1, 0].text(0.95, 0.95, \n",
        "               f'Skewness: {skewness:.3f}\\nKurtosis: {kurtosis:.3f}\\n(Normal: 0, 0)',\n",
        "               transform=axes[1, 0].transAxes,\n",
        "               verticalalignment='top',\n",
        "               horizontalalignment='right',\n",
        "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "               fontsize=9)\n",
        "\n",
        "# ============================================\n",
        "# 4. Residuals vs Order - Checking INDEPENDENCE (Critical for Time Series!)\n",
        "# ============================================\n",
        "axes[1, 1].scatter(all_order, all_residuals, alpha=0.6, s=30, c=all_order, \n",
        "                  cmap='viridis', edgecolors='black', linewidth=0.5)\n",
        "axes[1, 1].plot(all_order, all_residuals, 'b-', alpha=0.3, linewidth=1, label='Residuals')\n",
        "axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Residual Line')\n",
        "axes[1, 1].set_title('Residuals vs Observation Order\\n(Checking Independence/Autocorrelation)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Observation Order (Time Index)', fontsize=10)\n",
        "axes[1, 1].set_ylabel('Residuals', fontsize=10)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Test for autocorrelation (Durbin-Watson test)\n",
        "try:\n",
        "    dw_stat = durbin_watson(all_residuals)\n",
        "    axes[1, 1].text(0.05, 0.95, \n",
        "                   f'Durbin-Watson: {dw_stat:.4f}\\n(2.0 = no autocorr)',\n",
        "                   transform=axes[1, 1].transAxes,\n",
        "                   verticalalignment='top',\n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "                   fontsize=9)\n",
        "    \n",
        "    # Durbin-Watson interpretation: close to 2 = no autocorrelation\n",
        "    if 1.5 < dw_stat < 2.5:\n",
        "        axes[1, 1].text(0.95, 0.05, '✓ Independence OK', \n",
        "                       transform=axes[1, 1].transAxes,\n",
        "                       verticalalignment='bottom',\n",
        "                       horizontalalignment='right',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
        "                       fontsize=10, fontweight='bold')\n",
        "    else:\n",
        "        axes[1, 1].text(0.95, 0.05, '✗ Autocorrelation Detected', \n",
        "                       transform=axes[1, 1].transAxes,\n",
        "                       verticalalignment='bottom',\n",
        "                       horizontalalignment='right',\n",
        "                       bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7),\n",
        "                       fontsize=10, fontweight='bold')\n",
        "except ImportError:\n",
        "    dw_stat = None\n",
        "    # Alternative: Ljung-Box test\n",
        "    try:\n",
        "        lb_stat, lb_p = acorr_ljungbox(all_residuals, lags=10, return_df=False)\n",
        "        axes[1, 1].text(0.05, 0.95, \n",
        "                       f'Ljung-Box p-value: {lb_p[-1]:.4f}',\n",
        "                       transform=axes[1, 1].transAxes,\n",
        "                       verticalalignment='top',\n",
        "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "                       fontsize=9)\n",
        "    except:\n",
        "        autocorr = np.corrcoef(all_residuals[:-1], all_residuals[1:])[0, 1]\n",
        "        axes[1, 1].text(0.05, 0.95, \n",
        "                       f'Lag-1 Autocorr: {autocorr:.4f}',\n",
        "                       transform=axes[1, 1].transAxes,\n",
        "                       verticalalignment='top',\n",
        "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "                       fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESIDUAL DIAGNOSTIC SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n1. NORMALITY:\")\n",
        "print(f\"   - Mean of residuals: {np.mean(all_residuals):.4f} (should be ~0)\")\n",
        "print(f\"   - Skewness: {skewness:.4f} (should be ~0 for normality)\")\n",
        "print(f\"   - Kurtosis: {kurtosis:.4f} (should be ~0 for normality)\")\n",
        "print(f\"   - Shapiro-Wilk p-value: {shapiro_p:.4f} (>0.05 indicates normality)\")\n",
        "\n",
        "print(\"\\n2. LINEARITY & HOMOSCEDASTICITY:\")\n",
        "print(f\"   - Residuals should be randomly scattered around zero\")\n",
        "print(f\"   - No clear patterns or funnel shapes in residuals vs fitted plot\")\n",
        "if levene_p is not None:\n",
        "    print(f\"   - Levene test p-value: {levene_p:.4f} (>0.05 indicates constant variance)\")\n",
        "else:\n",
        "    print(\"   - Levene test p-value: N/A (test not performed)\")\n",
        "\n",
        "print(\"\\n3. INDEPENDENCE (Critical for Time Series):\")\n",
        "if dw_stat is not None:\n",
        "    print(f\"   - Durbin-Watson statistic: {dw_stat:.4f} (close to 2.0 = no autocorrelation)\")\n",
        "    if 1.5 < dw_stat < 2.5:\n",
        "        print(\"   - ✓ No significant autocorrelation detected\")\n",
        "    else:\n",
        "        print(\"   - ✗ Autocorrelation detected - model may not capture temporal dependencies\")\n",
        "else:\n",
        "    print(\"   - Check the residuals vs order plot for patterns\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Visualize Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot predictions vs actual\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Full time series with predictions\n",
        "axes[0].plot(range(len(train_actuals)), train_actuals, label='Actual (Train)', color='blue', alpha=0.7, linewidth=1.5)\n",
        "axes[0].plot(range(len(train_actuals)), train_predictions, label='Predicted (Train)', color='green', alpha=0.7, linewidth=1.5)\n",
        "axes[0].plot(range(len(train_actuals), len(train_actuals) + len(test_actuals)), test_actuals, \n",
        "             label='Actual (Test)', color='red', alpha=0.7, linewidth=1.5)\n",
        "axes[0].plot(range(len(train_actuals), len(train_actuals) + len(test_actuals)), test_predictions, \n",
        "             label='Predicted (Test)', color='orange', alpha=0.7, linewidth=1.5)\n",
        "axes[0].axvline(x=len(train_actuals), color='black', linestyle='--', linewidth=2, label='Train/Test Split')\n",
        "axes[0].set_title('Gray Model Time Series Predictions: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Time Index', fontsize=12)\n",
        "axes[0].set_ylabel('PM 2.5', fontsize=12)\n",
        "axes[0].legend(loc='best')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Scatter plot for test set\n",
        "axes[1].scatter(test_actuals, test_predictions, alpha=0.6, s=50)\n",
        "axes[1].plot([test_actuals.min(), test_actuals.max()], [test_actuals.min(), test_actuals.max()], \n",
        "             'r--', lw=2, label='Perfect Prediction')\n",
        "axes[1].set_xlabel('Actual PM 2.5', fontsize=12)\n",
        "axes[1].set_ylabel('Predicted PM 2.5', fontsize=12)\n",
        "axes[1].set_title(f'Test Set: Actual vs Predicted (R² = {test_r2:.4f})', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Experiment with Different Window Sizes\n",
        "\n",
        "Let's test different window sizes to find the optimal one for Gray Model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different window sizes (smaller windows for Gray Model)\n",
        "window_sizes = [5, 6, 7, 8, 10, 12, 15]\n",
        "results = []\n",
        "\n",
        "for ws in window_sizes:\n",
        "    try:\n",
        "        # Create windows with this size\n",
        "        windows_ws, targets_ws = create_sliding_windows_gm(ts, window_size=ws, forecast_horizon=1)\n",
        "        \n",
        "        # Split chronologically\n",
        "        split_idx = int(len(windows_ws) * 0.8)\n",
        "        train_windows_ws = windows_ws[:split_idx]\n",
        "        test_windows_ws = windows_ws[split_idx:]\n",
        "        train_targets_ws = targets_ws[:split_idx]\n",
        "        test_targets_ws = targets_ws[split_idx:]\n",
        "        \n",
        "        # Train and predict on training set\n",
        "        train_preds_ws = []\n",
        "        train_acts_ws = []\n",
        "        for window in train_windows_ws:\n",
        "            try:\n",
        "                gm = GrayModel()\n",
        "                gm.fit(window)\n",
        "                pred = gm.predict(steps=1, start=len(window)-1)[0]\n",
        "                train_preds_ws.append(pred)\n",
        "            except:\n",
        "                train_preds_ws.append(np.mean(window))\n",
        "        \n",
        "        for target in train_targets_ws:\n",
        "            train_acts_ws.append(target[0])\n",
        "        \n",
        "        # Predict on test set\n",
        "        test_preds_ws = []\n",
        "        test_acts_ws = []\n",
        "        for window in test_windows_ws:\n",
        "            try:\n",
        "                gm = GrayModel()\n",
        "                gm.fit(window)\n",
        "                forecast = gm.forecast(steps=1)[0]\n",
        "                test_preds_ws.append(forecast)\n",
        "            except:\n",
        "                test_preds_ws.append(np.mean(window))\n",
        "        \n",
        "        for target in test_targets_ws:\n",
        "            test_acts_ws.append(target[0])\n",
        "        \n",
        "        # Calculate metrics\n",
        "        train_r2_ws = r2_score(train_acts_ws, train_preds_ws)\n",
        "        test_r2_ws = r2_score(test_acts_ws, test_preds_ws)\n",
        "        test_rmse_ws = np.sqrt(mean_squared_error(test_acts_ws, test_preds_ws))\n",
        "        \n",
        "        results.append({\n",
        "            'Window Size': ws,\n",
        "            'Train R²': train_r2_ws,\n",
        "            'Test R²': test_r2_ws,\n",
        "            'Test RMSE': test_rmse_ws\n",
        "        })\n",
        "        \n",
        "        print(f\"Window Size {ws:2d}: Test R² = {test_r2_ws:.4f}, Test RMSE = {test_rmse_ws:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Window Size {ws:2d}: Failed - {str(e)}\")\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(results)\n",
        "if len(results_df) > 0:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SUMMARY OF RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(results_df.to_string(index=False))\n",
        "    \n",
        "    # Visualize window size comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    axes[0].plot(results_df['Window Size'], results_df['Train R²'], marker='o', label='Train R²', linewidth=2, markersize=8)\n",
        "    axes[0].plot(results_df['Window Size'], results_df['Test R²'], marker='s', label='Test R²', linewidth=2, markersize=8)\n",
        "    axes[0].set_xlabel('Window Size', fontsize=12)\n",
        "    axes[0].set_ylabel('R² Score', fontsize=12)\n",
        "    axes[0].set_title('R² Score vs Window Size', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1].plot(results_df['Window Size'], results_df['Test RMSE'], marker='o', color='red', linewidth=2, markersize=8)\n",
        "    axes[1].set_xlabel('Window Size', fontsize=12)\n",
        "    axes[1].set_ylabel('Test RMSE', fontsize=12)\n",
        "    axes[1].set_title('Test RMSE vs Window Size', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find best window size\n",
        "    best_window = results_df.loc[results_df['Test R²'].idxmax()]\n",
        "    print(f\"\\nBest Window Size: {int(best_window['Window Size'])}\")\n",
        "    print(f\"Best Test R²: {best_window['Test R²']:.4f}\")\n",
        "    print(f\"Best Test RMSE: {best_window['Test RMSE']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Summary and Conclusions\n",
        "\n",
        "### Key Findings:\n",
        "1. **Gray Model (GM(1,1))**: Successfully implemented for small dataset time series forecasting\n",
        "2. **Sliding Windows**: Applied to create features from past time steps\n",
        "3. **Window Size Optimization**: Tested multiple window sizes to find optimal configuration\n",
        "4. **Residual Diagnostics**: Comprehensive evaluation of model assumptions\n",
        "\n",
        "### Advantages of Gray Model:\n",
        "- Excellent for small datasets (typically 4-10+ data points)\n",
        "- Simple and computationally efficient\n",
        "- No strict assumptions about data distribution\n",
        "- Good for short-term forecasting\n",
        "- Works well with limited historical data\n",
        "\n",
        "### Model Performance:\n",
        "- Performance depends on the selected window size\n",
        "- Smaller windows (5-10) typically work best for Gray Model\n",
        "- Experiment with different window sizes to optimize results\n",
        "- Residual diagnostics help validate model assumptions\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
